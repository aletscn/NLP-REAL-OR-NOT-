{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "subwords.idpy",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOHGQozHSuQpYYwS+5pFuly",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aletscn/NLP-REAL-OR-NOT-/blob/master/subwords_idpy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "umW5pGbJLBCl",
        "colab_type": "text"
      },
      "source": [
        "BPE "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Q52GFom4AjQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "import os\n",
        "from keras.callbacks import TensorBoard\n",
        "import tensorflow as tf\n",
        "from math import log\n",
        "\n",
        "\n",
        "class BPE(object):\n",
        "\n",
        "    def __init__(self, vocab_file):\n",
        "        with open(vocab_file, encoding=\"utf8\") as f:\n",
        "            self.words = [l.split()[0] for l in f]\n",
        "            log_len = log(len(self.words))\n",
        "            self.wordcost = {\n",
        "                k: log((i+1) * log_len)\n",
        "                for i, k in enumerate(self.words)}\n",
        "            self.maxword = max(len(x) for x in self.words)\n",
        "\n",
        "    def encode(self, s):\n",
        "        \"\"\"Uses dynamic programming to infer the location of spaces in a string\n",
        "        without spaces.\"\"\"\n",
        "\n",
        "        s = s.replace(\" \", \"▁\")\n",
        "\n",
        "        # Find the best match for the i first characters, assuming cost has\n",
        "        # been built for the i-1 first characters.\n",
        "        # Returns a pair (match_cost, match_length).\n",
        "        def best_match(i):\n",
        "            candidates = enumerate(reversed(cost[max(0, i - self.maxword):i]))\n",
        "            return min(\n",
        "                (c + self.wordcost.get(s[i-k-1:i], 9e999), k+1)\n",
        "                for k, c in candidates)\n",
        "\n",
        "        # Build the cost array.\n",
        "        cost = [0]\n",
        "        for i in range(1, len(s) + 1):\n",
        "            c, k = best_match(i)\n",
        "            cost.append(c)\n",
        "\n",
        "        # Backtrack to recover the minimal-cost string.\n",
        "        out = []\n",
        "        i = len(s)\n",
        "        while i > 0:\n",
        "            c, k = best_match(i)\n",
        "            assert c == cost[i]\n",
        "            out.append(s[i-k:i])\n",
        "\n",
        "            i -= k\n",
        "\n",
        "        return \" \".join(reversed(out))\n",
        "\n",
        "\n",
        "#========plot train and validation scalars in a same figure=======\n",
        "class TrainValTensorBoard(TensorBoard):\n",
        "    def __init__(self, log_dir='./logs', **kwargs):\n",
        "        # Make the original `TensorBoard` log to a subdirectory 'training'\n",
        "        training_log_dir = os.path.join(log_dir, 'training')\n",
        "        super(TrainValTensorBoard, self).__init__(training_log_dir, **kwargs)\n",
        "\n",
        "        # Log the validation metrics to a separate subdirectory\n",
        "        self.val_log_dir = os.path.join(log_dir, 'validation')\n",
        "\n",
        "    def set_model(self, model):\n",
        "        # Setup writer for validation metrics\n",
        "        self.val_writer = tf.summary.FileWriter(self.val_log_dir)\n",
        "        super(TrainValTensorBoard, self).set_model(model)\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        # Pop the validation logs and handle them separately with\n",
        "        # `self.val_writer`. Also rename the keys so that they can\n",
        "        # be plotted on the same figure with the training metrics\n",
        "        logs = logs or {}\n",
        "        val_logs = {k.replace('val_', ''): v for k, v in logs.items() if k.startswith('val_')}\n",
        "        for name, value in val_logs.items():\n",
        "            summary = tf.Summary()\n",
        "            summary_value = summary.value.add()\n",
        "            summary_value.simple_value = value.item()\n",
        "            summary_value.tag = name\n",
        "            self.val_writer.add_summary(summary, epoch)\n",
        "        self.val_writer.flush()\n",
        "\n",
        "        # Pass the remaining logs to `TensorBoard.on_epoch_end`\n",
        "        logs = {k: v for k, v in logs.items() if not k.startswith('val_')}\n",
        "        super(TrainValTensorBoard, self).on_epoch_end(epoch, logs)\n",
        "\n",
        "    def on_train_end(self, logs=None):\n",
        "        super(TrainValTensorBoard, self).on_train_end(logs)\n",
        "        self.val_writer.close()\n",
        "import numpy as np\n",
        "import re\n",
        "import itertools\n",
        "from collections import Counter\n",
        "def clean_str(string):\n",
        "    \"\"\"\n",
        "    Tokenization/string cleaning for all datasets except for SST.\n",
        "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
        "    \"\"\"\n",
        "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
        "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
        "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
        "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
        "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
        "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
        "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
        "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
        "    string = re.sub(r\",\", \" , \", string)\n",
        "    string = re.sub(r\"!\", \" ! \", string)\n",
        "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
        "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
        "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
        "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
        "    return string.strip().lower()\n",
        "\n",
        "\n",
        "def load_data_and_labels(positive_data_file, negtive_data_file):\n",
        "    \"\"\"\n",
        "    Load data from files, split data into words and generate labels\n",
        "    Input: the positive data file path and negative data file path\n",
        "    Output:\n",
        "        x_text: list of words for sentences. e.g [['i', 'am', is'], ['word', 'is', 'too', 'long'], ...,]\n",
        "        y: For each sentence, using `[neg, pos]` to represent the lables.\n",
        "           - If we have a positive label, we represent it as `[0, 1]`\n",
        "           - If we have a negative label, we represent it as `[1, 0]`\n",
        "    \"\"\"\n",
        "\n",
        "    # Load data from files\n",
        "    positive_examples = list(open(positive_data_file, 'r', encoding='utf-8').readlines())\n",
        "    positive_examples = [s.strip() for s in positive_examples]\n",
        "    negative_examples = list(open(negtive_data_file, 'r', encoding='utf-8').readlines())\n",
        "    negative_examples = [s.strip() for s in negative_examples]\n",
        "\n",
        "    # Split by words\n",
        "    x_text = positive_examples + negative_examples\n",
        "    x_text = [clean_str(sen) for sen in x_text]\n",
        "    # x_text = [sen.split(\" \") for sen in x_text]\n",
        "\n",
        "    # Generate labels\n",
        "    positive_lables = [[0, 1] for _ in positive_examples]\n",
        "    negative_lables = [[1, 0] for _ in negative_examples]\n",
        "    y = np.concatenate((positive_lables, negative_lables), 0)\n",
        "    return x_text, y\n",
        "\n",
        "\n",
        "def pad_sentences(sentences, padding_word='<PAD/>'):\n",
        "    \"\"\"\n",
        "    :param sentences: sentences as list of words,  [['i', 'am', is'], ['word', 'is', 'too', 'long'], ...,]\n",
        "    :return: pad sentence to longest length, [['i', 'am', is', '<PAD>', '<PAD>'], ['word', 'is', 'too', 'long', '<PAD>'], ...,]\n",
        "    \"\"\"\n",
        "    sequence_length = max(len(sen) for sen in sentences)\n",
        "    padded_sentences = []\n",
        "    for i in range(len(sentences)):\n",
        "        sentence = sentences[i]\n",
        "        num_padding = sequence_length - len(sentence)\n",
        "        new_sentence = sentence + [padding_word] * num_padding\n",
        "        padded_sentences.append(new_sentence)\n",
        "    return padded_sentences\n",
        "\n",
        "\n",
        "def build_vocab(sentences):\n",
        "    \"\"\"\n",
        "    :param sentences:  sentences after padding\n",
        "    :return:\n",
        "        vocabulary: a dict object, key is word and value is index. e.g. {'i': 0, 'am':1}\n",
        "        vocabulary_inv: a dict object, the inverse of vocabulary. e.g. {0: 'i', 1:'am'}\n",
        "    \"\"\"\n",
        "    # Count words\n",
        "    word_counts = Counter(itertools.chain(*sentences))\n",
        "    # Sort the word as frequency order\n",
        "    vocabulay_inv = [x[0] for x in word_counts.most_common()]\n",
        "    # Build vocabulary, word: index\n",
        "    vocabulay = {word: i for i, word in enumerate(vocabulay_inv)}\n",
        "    # Build inverse vocabulary, index: word\n",
        "    vocabulay_inv = {value: key for key, value in vocabulay.items()}\n",
        "\n",
        "    return [vocabulay, vocabulay_inv]\n",
        "\n",
        "def build_index_sentence(sentences, vocabulary):\n",
        "    # x = []\n",
        "    # for sen in sentences:\n",
        "    #     one_sen = []\n",
        "    #     for word in sen:\n",
        "    #         one_sen.append(vocabulary[word])\n",
        "    #     x.append(one_sen)\n",
        "    # return np.array(x)\n",
        "\n",
        "    # write above code as one line\n",
        "    x = np.array([[vocabulary[word] for word in sen] for sen in sentences])\n",
        "    return x"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FcZdj9ovrrda",
        "colab_type": "text"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eEirG7R2LFtl",
        "colab_type": "text"
      },
      "source": [
        "Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M9AsXhdhBkwn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "from keras.layers import Input, Embedding, Activation, Flatten, Dense, Concatenate, Bidirectional, LeakyReLU,SpatialDropout1D\n",
        "from keras import regularizers\n",
        "from keras.optimizers import Adam\n",
        "from keras.layers import Conv1D, MaxPooling1D, Dropout, LSTM\n",
        "from keras.models import Model, Sequential\n",
        "from keras.callbacks import CSVLogger\n",
        "from sklearn.model_selection import GridSearchCV, cross_validate\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "udQl-t6XLJlK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "c41c2159-e44e-4d81-d027-b6ddc26e349a"
      },
      "source": [
        "#Para poder abrir cosas en el Drive\n",
        "import sys\n",
        "#Donde se encuentran los documentos (train, text)\n",
        "DRIVE_DIR='/content/drive'\n",
        "BASE_DIR=''\n",
        "DATA_DIR = 'drive/My Drive/Colab Notebooks'\n",
        "TWEETS_DIR='data_tp2'\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount(DRIVE_DIR)\n",
        "\n",
        "os.chdir(DATA_DIR)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1KO7S3i_I1OY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train=pd.read_csv(os.path.join(TWEETS_DIR, 'train_clean.csv'))\n",
        "train=train.fillna(' ')\n",
        "\n",
        "test=pd.read_csv(os.path.join(TWEETS_DIR, 'test_clean.csv'))\n",
        "test=test.fillna(' ')\n",
        "\n",
        "real_test=pd.read_csv(os.path.join(TWEETS_DIR, 'submission.csv'))\n",
        "\n",
        "x_text = train['text_clean_nosw']\n",
        "y = train['target'].values\n",
        "y = to_categorical(y)\n",
        "\n",
        "x_test = test['text_clean_nosw']\n",
        "id_test=test['id']\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KY2mIAf95EnS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert subword to index, function version\n",
        "def subword2index(texts, vocab):\n",
        "    sentences = []\n",
        "    for s in texts:\n",
        "        s = s.split()\n",
        "        one_line = []\n",
        "        for word in s:\n",
        "            if word not in vocab.keys():\n",
        "                one_line.append(vocab['unk'])\n",
        "            else:\n",
        "                one_line.append(vocab[word])\n",
        "        sentences.append(one_line)\n",
        "    return sentences\n",
        "\n",
        "\n",
        "# replace all digits with 0\n",
        "import re\n",
        "\n",
        "\n",
        "#bpe = BPE(\"./pre-trained-model/en.wiki.bpe.op25000.vocab.txt\")\n",
        "bpe = BPE(\"./pre-trained-model/en.wiki.bpe.vs100000.vocab.txt\")\n",
        "train_texts = [bpe.encode(s) for s in x_text]\n",
        "test_texts = [bpe.encode(s) for s in x_test]\n",
        "\n",
        "# Build vocab, {token: index}\n",
        "vocab = {}\n",
        "for i, token in enumerate(bpe.words):\n",
        "    vocab[token] = i + 1\n",
        "\n",
        "# Convert train and test\n",
        "train_sentences = subword2index(train_texts, vocab)\n",
        "test_sentences = subword2index(test_texts, vocab)\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B6GEOpTE5T8t",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "bceda730-ebf1-4abb-edfd-024d0def0127"
      },
      "source": [
        "\n",
        "# See subword level length\n",
        "length = [len(sent) for sent in train_sentences]\n",
        "print('The max length is: ', max(length))\n",
        "print('The min length is: ', min(length))\n",
        "print('The average length is: ', sum(length)/len(length))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The max length is:  91\n",
            "The min length is:  1\n",
            "The average length is:  10.144420131291028\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZTU8SdgW8mPI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "f0836d6d-efb9-4c93-ecc0-9a1f928f5897"
      },
      "source": [
        "\n",
        "# Padding\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "train_data = pad_sequences(train_sentences, maxlen=max(length), padding='post')\n",
        "test_data = pad_sequences(test_sentences, maxlen=max(length), padding='post')\n",
        "\n",
        "x_train=train_data\n",
        "y_train=y\n",
        "x_test=test_data\n",
        "\n",
        "#Adding numerical features\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
        "\n",
        "#define and scale our added features\n",
        "meta_train = StandardScaler().fit_transform(train.iloc[:, 9:])\n",
        "meta_test = StandardScaler().fit_transform(test.iloc[:, 8:])\n",
        "\n",
        "\n",
        "#meta_train = MinMaxScaler().fit_transform(train.iloc[:, 2:])\n",
        "#meta_test = MinMaxScaler().fit_transform(test.iloc[:, 2:])\n",
        "\n",
        "#meta_train = RobustScaler().fit_transform(train.iloc[:, 9:])\n",
        "#meta_test = RobustScaler().fit_transform(test.iloc[:, 8:])\n",
        "\n",
        "\n",
        "# Embedding Initialization\n",
        "from gensim.models import KeyedVectors\n",
        "model = KeyedVectors.load_word2vec_format(\"./pre-trained-model/en.wiki.bpe.vs100000.d200.w2v.bin\", binary=True)\n",
        "from keras.layers import Embedding\n",
        "\n",
        "input_size = max(length)\n",
        "embedding_dim = 200\n",
        "embedding_weights = np.zeros((len(vocab) + 1, embedding_dim)) # (25001, 50)\n",
        "\n",
        "\n",
        "for subword, i in vocab.items():\n",
        "    if subword in model.vocab:\n",
        "        embedding_vector = model[subword]\n",
        "        if embedding_vector is not None:\n",
        "            embedding_weights[i] = embedding_vector\n",
        "    else:\n",
        "        continue\n",
        "\n",
        "embedding_layer = Embedding(len(vocab)+1,\n",
        "                            embedding_dim,\n",
        "                            weights=[embedding_weights],\n",
        "                            input_length=input_size,\n",
        "                            trainable=False)\n",
        "\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:254: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qCzO8Y_YgHQW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zb4XyR4sdowF",
        "colab_type": "text"
      },
      "source": [
        "LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zxeT3CMfgOjt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#function to create lstm model\n",
        "def create_lstm(spatial_dropout, dropout, recurrent_dropout, learning_rate, bidirectional):\n",
        "    #define activation\n",
        "    activation = LeakyReLU()\n",
        "    \n",
        "    #define inputs\n",
        "    nlp_input = Input(shape = (max(length),), name = 'nlp_input')\n",
        "    meta_input_train = Input(shape = (12, ), name = 'meta_train')\n",
        "    emb = embedding_layer(nlp_input)\n",
        "    #emb = SpatialDropout1D(dropout)(emb)\n",
        "\n",
        "\n",
        "    #add LSTM layer\n",
        "    if bidirectional:\n",
        "        nlp_out = (Bidirectional(LSTM(128, dropout = dropout, recurrent_dropout = recurrent_dropout,\n",
        "                                kernel_initializer = 'orthogonal')))(emb)\n",
        "    else:\n",
        "        nlp_out = (LSTM(128, dropout = dropout, recurrent_dropout = recurrent_dropout,\n",
        "                      kernel_initializer = 'orthogonal'))(emb)\n",
        "     \n",
        "    #add meta data    \n",
        "    x = Concatenate()([nlp_out, meta_input_train])\n",
        "    \n",
        "\n",
        "    #add second hidden layer\n",
        "    x = Dropout(dropout)(x)\n",
        "    x = (Dense(100, activation = activation, kernel_regularizer = regularizers.l2(1e-4),\n",
        "              kernel_initializer = 'he_normal'))(x)\n",
        "    \n",
        "    #add output layer\n",
        "    x = Dropout(dropout)(x)\n",
        "    preds = Dense(2, activation='softmax', kernel_regularizer = regularizers.l2(1e-4))(x)\n",
        "    \n",
        "    #compile model\n",
        "    model = Model(inputs=[nlp_input , meta_input_train], outputs = preds)\n",
        "    optimizer = Adam(learning_rate = learning_rate)\n",
        "    model.compile(loss = 'binary_crossentropy', optimizer = optimizer, metrics = ['accuracy'])\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ys85Wts2dtS3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 523
        },
        "outputId": "b817430c-903e-48b5-e256-2c7b324c7654"
      },
      "source": [
        "#define new model\n",
        "lstm = create_lstm(spatial_dropout = .4, dropout = .4, recurrent_dropout = .4,\n",
        "                       learning_rate = 0.0001, bidirectional = False)\n",
        "\n",
        "lstm.summary()\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "Model: \"functional_3\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "nlp_input (InputLayer)          [(None, 91)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Embedding)           (None, 91, 200)      20000200    nlp_input[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lstm_2 (LSTM)                   (None, 128)          168448      embedding[1][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "meta_train (InputLayer)         [(None, 12)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 140)          0           lstm_2[0][0]                     \n",
            "                                                                 meta_train[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_2 (Dropout)             (None, 140)          0           concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 100)          14100       dropout_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_3 (Dropout)             (None, 100)          0           dense_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 2)            202         dropout_3[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 20,182,950\n",
            "Trainable params: 182,750\n",
            "Non-trainable params: 20,000,200\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cN5xflkrdvoO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "outputId": "653fb904-85ed-4c34-e056-6972ecace5a6"
      },
      "source": [
        "mc = ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True,save_weights_only = True)\n",
        "#callback = EarlyStopping(monitor = 'val_loss', patience = 4)\n",
        "history2 = lstm.fit([x_train, meta_train], y_train, validation_split = .2,\n",
        "                       epochs = 10, batch_size = 128, verbose = 1, callbacks = [mc])"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "43/43 [==============================] - 13s 310ms/step - loss: 0.5171 - accuracy: 0.7757 - val_loss: 0.4515 - val_accuracy: 0.8162\n",
            "Epoch 2/10\n",
            "43/43 [==============================] - 14s 321ms/step - loss: 0.5087 - accuracy: 0.7832 - val_loss: 0.4474 - val_accuracy: 0.8096\n",
            "Epoch 3/10\n",
            "43/43 [==============================] - 13s 305ms/step - loss: 0.5114 - accuracy: 0.7806 - val_loss: 0.4486 - val_accuracy: 0.8104\n",
            "Epoch 4/10\n",
            "43/43 [==============================] - 13s 305ms/step - loss: 0.5025 - accuracy: 0.7861 - val_loss: 0.4499 - val_accuracy: 0.8140\n",
            "Epoch 5/10\n",
            "43/43 [==============================] - 13s 299ms/step - loss: 0.5013 - accuracy: 0.7883 - val_loss: 0.4487 - val_accuracy: 0.8133\n",
            "Epoch 6/10\n",
            "43/43 [==============================] - 13s 309ms/step - loss: 0.4928 - accuracy: 0.7947 - val_loss: 0.4465 - val_accuracy: 0.8096\n",
            "Epoch 7/10\n",
            "43/43 [==============================] - 13s 306ms/step - loss: 0.5045 - accuracy: 0.7907 - val_loss: 0.4524 - val_accuracy: 0.8125\n",
            "Epoch 8/10\n",
            "43/43 [==============================] - 13s 303ms/step - loss: 0.4919 - accuracy: 0.7936 - val_loss: 0.4468 - val_accuracy: 0.8191\n",
            "Epoch 9/10\n",
            "43/43 [==============================] - 14s 319ms/step - loss: 0.4883 - accuracy: 0.7954 - val_loss: 0.4453 - val_accuracy: 0.8162\n",
            "Epoch 10/10\n",
            "43/43 [==============================] - 14s 321ms/step - loss: 0.4925 - accuracy: 0.7914 - val_loss: 0.4428 - val_accuracy: 0.8118\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B2rMoagReFaL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "806aee29-5a18-486d-a550-8daeff0b4131"
      },
      "source": [
        "lstm.load_weights('best_model.h5')\n",
        "y_pred = lstm.predict([x_test, meta_test], batch_size=16, verbose=2)\n",
        "y_pred = np.argmax(y_pred,axis=1)\n",
        "f1_score(real_test['target'], y_pred,average='micro')\n",
        "\n",
        "\n",
        "#df = pd.DataFrame()\n",
        "#df['id'] = [int(x) for x in id_test.values]\n",
        "#df['target'] = y_pred\n",
        "#df.to_csv('lstm.csv', index=False)\n",
        "#from google.colab import files\n",
        "#files.download('lstm.csv')"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "204/204 - 5s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8023291449586272"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSe8-7jWLjHj",
        "colab_type": "text"
      },
      "source": [
        "LSTM Bidireccional"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j75lqikDKXIs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#function to create lstm model\n",
        "def create_lstm_2(spatial_dropout, dropout, recurrent_dropout, learning_rate, bidirectional):\n",
        "    #define activation\n",
        "    activation = LeakyReLU(alpha = 0.001)\n",
        "    \n",
        "    #define inputs\n",
        "    nlp_input = Input(shape = (max(length),), name = 'nlp_input')\n",
        "    meta_input_train = Input(shape = (12, ), name = 'meta_train')\n",
        "    emb = embedding_layer(nlp_input)\n",
        "\n",
        "    #add LSTM layer\n",
        "    if bidirectional:\n",
        "        nlp_out = (Bidirectional(LSTM(128, dropout = dropout, recurrent_dropout = recurrent_dropout,\n",
        "                                kernel_initializer = 'orthogonal', return_sequences = True)))(emb)\n",
        "        nlp_out = (Bidirectional(LSTM(128, dropout = dropout, recurrent_dropout = recurrent_dropout,\n",
        "                                 kernel_initializer = 'orthogonal')))(nlp_out)\n",
        "    else:\n",
        "        nlp_out = (LSTM(128, dropout = dropout, recurrent_dropout = recurrent_dropout,\n",
        "                                 kernel_initializer = 'orthogonal', return_sequences = True))(emb)\n",
        "        nlp_out = (LSTM(128, dropout = dropout, recurrent_dropout = recurrent_dropout,\n",
        "                                 kernel_initializer = 'orthogonal'))(nlp_out)\n",
        "     \n",
        "    #add meta data    \n",
        "    x = Concatenate()([nlp_out, meta_input_train])\n",
        "    \n",
        "    #add second hidden layer\n",
        "    x = Dropout(dropout)(x)\n",
        "    x = (Dense(100, activation = activation, kernel_regularizer = regularizers.l2(1e-4),\n",
        "              kernel_initializer = 'he_normal'))(x)\n",
        "    \n",
        "    #add output layer\n",
        "    x = Dropout(dropout)(x)\n",
        "    preds = Dense(2, activation='softmax', kernel_regularizer = regularizers.l2(1e-4))(x)\n",
        "    \n",
        "    #compile model\n",
        "    model = Model(inputs=[nlp_input , meta_input_train], outputs = preds)\n",
        "    optimizer = Adam(learning_rate = learning_rate)\n",
        "    model.compile(loss = 'binary_crossentropy', optimizer = optimizer, metrics = ['accuracy'])\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1a4RtwXZJl8G",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 644
        },
        "outputId": "6b4cd12d-373c-449d-cefb-bd7687be4ac2"
      },
      "source": [
        "\n",
        "#define new model\n",
        "lstm_2 = create_lstm_2(spatial_dropout = .4, dropout = .4, recurrent_dropout = .4,\n",
        "                       learning_rate = 0.0001, bidirectional = True)\n",
        "\n",
        "lstm_2.summary()"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer lstm_13 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer lstm_13 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer lstm_13 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer lstm_14 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer lstm_14 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer lstm_14 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "Model: \"functional_19\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "nlp_input (InputLayer)          [(None, 91)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Embedding)           (None, 91, 200)      20000200    nlp_input[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_12 (Bidirectional (None, 91, 256)      336896      embedding[11][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_13 (Bidirectional (None, 256)          394240      bidirectional_12[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "meta_train (InputLayer)         [(None, 12)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_11 (Concatenate)    (None, 268)          0           bidirectional_13[0][0]           \n",
            "                                                                 meta_train[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_18 (Dropout)            (None, 268)          0           concatenate_11[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dense_18 (Dense)                (None, 100)          26900       dropout_18[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_19 (Dropout)            (None, 100)          0           dense_18[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_19 (Dense)                (None, 2)            202         dropout_19[0][0]                 \n",
            "==================================================================================================\n",
            "Total params: 20,758,438\n",
            "Trainable params: 758,238\n",
            "Non-trainable params: 20,000,200\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YoPAOVb1LrXt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 711
        },
        "outputId": "a9269eef-ce7c-49a6-bb86-56438ca1939f"
      },
      "source": [
        "#Me quiero quedar con el mejor epoch\n",
        "mc = ModelCheckpoint('best_model_lstm2.h5', monitor='val_loss', save_best_only=True,save_weights_only = True)\n",
        "#callback = EarlyStopping(monitor = 'val_loss', patience = 4)\n",
        "history2 = lstm_2.fit([x_train, meta_train], y_train, validation_split = .2,\n",
        "                       epochs = 20, batch_size = 128, verbose = 1, callbacks = [mc])"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "43/43 [==============================] - 54s 1s/step - loss: 0.4703 - accuracy: 0.8032 - val_loss: 0.4435 - val_accuracy: 0.8125\n",
            "Epoch 2/20\n",
            "43/43 [==============================] - 54s 1s/step - loss: 0.4633 - accuracy: 0.8074 - val_loss: 0.4445 - val_accuracy: 0.8089\n",
            "Epoch 3/20\n",
            "43/43 [==============================] - 54s 1s/step - loss: 0.4649 - accuracy: 0.8053 - val_loss: 0.4399 - val_accuracy: 0.8096\n",
            "Epoch 4/20\n",
            "43/43 [==============================] - 54s 1s/step - loss: 0.4582 - accuracy: 0.8102 - val_loss: 0.4399 - val_accuracy: 0.8096\n",
            "Epoch 5/20\n",
            "43/43 [==============================] - 54s 1s/step - loss: 0.4558 - accuracy: 0.8140 - val_loss: 0.4394 - val_accuracy: 0.8118\n",
            "Epoch 6/20\n",
            "43/43 [==============================] - 53s 1s/step - loss: 0.4443 - accuracy: 0.8149 - val_loss: 0.4407 - val_accuracy: 0.8133\n",
            "Epoch 7/20\n",
            "43/43 [==============================] - 53s 1s/step - loss: 0.4468 - accuracy: 0.8093 - val_loss: 0.4452 - val_accuracy: 0.8082\n",
            "Epoch 8/20\n",
            "43/43 [==============================] - 53s 1s/step - loss: 0.4447 - accuracy: 0.8116 - val_loss: 0.4416 - val_accuracy: 0.8125\n",
            "Epoch 9/20\n",
            "43/43 [==============================] - 53s 1s/step - loss: 0.4434 - accuracy: 0.8160 - val_loss: 0.4407 - val_accuracy: 0.8111\n",
            "Epoch 10/20\n",
            "43/43 [==============================] - 52s 1s/step - loss: 0.4412 - accuracy: 0.8184 - val_loss: 0.4433 - val_accuracy: 0.8089\n",
            "Epoch 11/20\n",
            "43/43 [==============================] - 53s 1s/step - loss: 0.4301 - accuracy: 0.8122 - val_loss: 0.4432 - val_accuracy: 0.8140\n",
            "Epoch 12/20\n",
            "43/43 [==============================] - 53s 1s/step - loss: 0.4338 - accuracy: 0.8156 - val_loss: 0.4445 - val_accuracy: 0.8053\n",
            "Epoch 13/20\n",
            "43/43 [==============================] - 52s 1s/step - loss: 0.4374 - accuracy: 0.8162 - val_loss: 0.4406 - val_accuracy: 0.8096\n",
            "Epoch 14/20\n",
            "43/43 [==============================] - 54s 1s/step - loss: 0.4342 - accuracy: 0.8175 - val_loss: 0.4431 - val_accuracy: 0.8111\n",
            "Epoch 15/20\n",
            "43/43 [==============================] - 54s 1s/step - loss: 0.4242 - accuracy: 0.8175 - val_loss: 0.4429 - val_accuracy: 0.8169\n",
            "Epoch 16/20\n",
            "43/43 [==============================] - 52s 1s/step - loss: 0.4265 - accuracy: 0.8228 - val_loss: 0.4408 - val_accuracy: 0.8155\n",
            "Epoch 17/20\n",
            "43/43 [==============================] - 54s 1s/step - loss: 0.4246 - accuracy: 0.8206 - val_loss: 0.4425 - val_accuracy: 0.8096\n",
            "Epoch 18/20\n",
            "43/43 [==============================] - 54s 1s/step - loss: 0.4170 - accuracy: 0.8233 - val_loss: 0.4437 - val_accuracy: 0.8104\n",
            "Epoch 19/20\n",
            "43/43 [==============================] - 53s 1s/step - loss: 0.4192 - accuracy: 0.8191 - val_loss: 0.4407 - val_accuracy: 0.8169\n",
            "Epoch 20/20\n",
            "43/43 [==============================] - 53s 1s/step - loss: 0.4137 - accuracy: 0.8242 - val_loss: 0.4457 - val_accuracy: 0.8111\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pJYEGRHenzU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "bb5fe2d8-9fec-4c0a-bb38-1dbfe20eaa3e"
      },
      "source": [
        "lstm_2.load_weights('best_model_lstm2.h5')\n",
        "y_pred = lstm_2.predict([x_test, meta_test], batch_size=16, verbose=2)\n",
        "y_pred = np.argmax(y_pred,axis=1)\n",
        "\n",
        "f1_score(real_test['target'], y_pred,average='micro')\n",
        "\n",
        "#df = pd.DataFrame()\n",
        "#df['id'] = [int(x) for x in id_test.values]\n",
        "#df['target'] = y_pred\n",
        "#df.to_csv('lstm.csv', index=False)\n",
        "#from google.colab import files\n",
        "#files.download('lstm.csv')"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "204/204 - 18s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8026356114005516"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08vxd8AtyRri",
        "colab_type": "text"
      },
      "source": [
        "CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6LQsANT1lKJM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#function to create cnn model\n",
        "def create_cnn(dropout, learning_rate):\n",
        "    #define activation\n",
        "    activation = LeakyReLU()\n",
        "\n",
        "    #define inputs\n",
        "    nlp_input = Input(shape = (max(length),), name = 'nlp_input')\n",
        "    meta_input_train = Input(shape = (12, ), name = 'meta_train')\n",
        "    emb = embedding_layer(nlp_input)\n",
        "\n",
        "    #x = Conv1D(32, 6, activation=activation,padding='valid')(emb)\n",
        "    #x = MaxPooling1D(2)(x)\n",
        "    #x = Dropout(dropout)(x)\n",
        "    x = Conv1D(128, 4, activation=activation,padding='valid')(emb)\n",
        "    x = MaxPooling1D(2)(x)\n",
        "    x = Dropout(dropout)(x)\n",
        "    x = Conv1D(64, 2, activation=activation,padding='valid')(x)\n",
        "    x = MaxPooling1D()(x) \n",
        "    x = Dropout(dropout)(x)\n",
        "    nlp_out = Flatten()(x)\n",
        "    x = Concatenate()([nlp_out, meta_input_train])\n",
        "    x = Dense(35, activation='relu')(x)\n",
        "    preds = Dense(2, activation='softmax', kernel_regularizer = regularizers.l2(1e-4))(x)\n",
        "\n",
        "\n",
        "    #compile model\n",
        "    model = Model(inputs=[nlp_input , meta_input_train], outputs = preds)\n",
        "    optimizer = Adam(learning_rate = learning_rate)\n",
        "    model.compile(loss = 'binary_crossentropy', optimizer = optimizer, metrics = ['accuracy'])\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WKXva9J3o5Kf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 624
        },
        "outputId": "a8cda608-3206-486b-c682-858d1bb9586e"
      },
      "source": [
        "#define new model\n",
        "cnn = create_cnn(dropout = .5, learning_rate = 0.0001)\n",
        "\n",
        "cnn.summary()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_9\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "nlp_input (InputLayer)          [(None, 91)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Embedding)           (None, 91, 200)      20000200    nlp_input[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, 88, 128)      102528      embedding[4][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_2 (MaxPooling1D)  (None, 44, 128)      0           conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_8 (Dropout)             (None, 44, 128)      0           max_pooling1d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, 43, 64)       16448       dropout_8[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_3 (MaxPooling1D)  (None, 21, 64)       0           conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_9 (Dropout)             (None, 21, 64)       0           max_pooling1d_3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "flatten_1 (Flatten)             (None, 1344)         0           dropout_9[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "meta_train (InputLayer)         [(None, 12)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_4 (Concatenate)     (None, 1356)         0           flatten_1[0][0]                  \n",
            "                                                                 meta_train[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_8 (Dense)                 (None, 35)           47495       concatenate_4[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_9 (Dense)                 (None, 2)            72          dense_8[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 20,166,743\n",
            "Trainable params: 166,543\n",
            "Non-trainable params: 20,000,200\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CD4z7cMMpKW3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6d73dc49-831e-49cc-c306-63abce133724"
      },
      "source": [
        "#Me quiero quedar con el mejor epoch\n",
        "mc_cnn = ModelCheckpoint('best_model_cnn.h5', monitor='val_accuracy', mode='max', save_best_only=True)\n",
        "#callback = EarlyStopping(monitor = 'val_loss', patience = 4)\n",
        "history2 = cnn.fit([x_train, meta_train], y_train, validation_split = .2,\n",
        "                       epochs = 30, batch_size = 32, verbose = 1, shuffle=True, callbacks = [mc_cnn])"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "172/172 [==============================] - 2s 10ms/step - loss: 0.6555 - accuracy: 0.6167 - val_loss: 0.6042 - val_accuracy: 0.6995\n",
            "Epoch 2/30\n",
            "172/172 [==============================] - 3s 15ms/step - loss: 0.5893 - accuracy: 0.7011 - val_loss: 0.5319 - val_accuracy: 0.7454\n",
            "Epoch 3/30\n",
            "172/172 [==============================] - 1s 7ms/step - loss: 0.5256 - accuracy: 0.7547 - val_loss: 0.4763 - val_accuracy: 0.7761\n",
            "Epoch 4/30\n",
            "172/172 [==============================] - 1s 7ms/step - loss: 0.4995 - accuracy: 0.7684 - val_loss: 0.4581 - val_accuracy: 0.7856\n",
            "Epoch 5/30\n",
            "172/172 [==============================] - 1s 5ms/step - loss: 0.4720 - accuracy: 0.7870 - val_loss: 0.4495 - val_accuracy: 0.7856\n",
            "Epoch 6/30\n",
            "172/172 [==============================] - 2s 10ms/step - loss: 0.4617 - accuracy: 0.7943 - val_loss: 0.4442 - val_accuracy: 0.7907\n",
            "Epoch 7/30\n",
            "172/172 [==============================] - 1s 7ms/step - loss: 0.4594 - accuracy: 0.7943 - val_loss: 0.4412 - val_accuracy: 0.7943\n",
            "Epoch 8/30\n",
            "172/172 [==============================] - 3s 15ms/step - loss: 0.4437 - accuracy: 0.8047 - val_loss: 0.4386 - val_accuracy: 0.7965\n",
            "Epoch 9/30\n",
            "172/172 [==============================] - 3s 15ms/step - loss: 0.4376 - accuracy: 0.8054 - val_loss: 0.4385 - val_accuracy: 0.7972\n",
            "Epoch 10/30\n",
            "172/172 [==============================] - 1s 7ms/step - loss: 0.4318 - accuracy: 0.8065 - val_loss: 0.4373 - val_accuracy: 0.8009\n",
            "Epoch 11/30\n",
            "172/172 [==============================] - 1s 5ms/step - loss: 0.4236 - accuracy: 0.8120 - val_loss: 0.4352 - val_accuracy: 0.7980\n",
            "Epoch 12/30\n",
            "172/172 [==============================] - 1s 5ms/step - loss: 0.4136 - accuracy: 0.8200 - val_loss: 0.4353 - val_accuracy: 0.8009\n",
            "Epoch 13/30\n",
            "172/172 [==============================] - 1s 7ms/step - loss: 0.4024 - accuracy: 0.8235 - val_loss: 0.4340 - val_accuracy: 0.8038\n",
            "Epoch 14/30\n",
            "172/172 [==============================] - 1s 5ms/step - loss: 0.4009 - accuracy: 0.8226 - val_loss: 0.4345 - val_accuracy: 0.8023\n",
            "Epoch 15/30\n",
            "172/172 [==============================] - 1s 5ms/step - loss: 0.3882 - accuracy: 0.8315 - val_loss: 0.4371 - val_accuracy: 0.8016\n",
            "Epoch 16/30\n",
            "172/172 [==============================] - 1s 5ms/step - loss: 0.3830 - accuracy: 0.8357 - val_loss: 0.4342 - val_accuracy: 0.8016\n",
            "Epoch 17/30\n",
            "172/172 [==============================] - 1s 5ms/step - loss: 0.3755 - accuracy: 0.8375 - val_loss: 0.4351 - val_accuracy: 0.8023\n",
            "Epoch 18/30\n",
            "172/172 [==============================] - 1s 6ms/step - loss: 0.3702 - accuracy: 0.8445 - val_loss: 0.4349 - val_accuracy: 0.8053\n",
            "Epoch 19/30\n",
            "172/172 [==============================] - 1s 5ms/step - loss: 0.3610 - accuracy: 0.8456 - val_loss: 0.4389 - val_accuracy: 0.7994\n",
            "Epoch 20/30\n",
            "172/172 [==============================] - 1s 5ms/step - loss: 0.3596 - accuracy: 0.8452 - val_loss: 0.4369 - val_accuracy: 0.8009\n",
            "Epoch 21/30\n",
            "172/172 [==============================] - 1s 5ms/step - loss: 0.3416 - accuracy: 0.8567 - val_loss: 0.4394 - val_accuracy: 0.8023\n",
            "Epoch 22/30\n",
            "172/172 [==============================] - 1s 5ms/step - loss: 0.3425 - accuracy: 0.8556 - val_loss: 0.4410 - val_accuracy: 0.8031\n",
            "Epoch 23/30\n",
            "172/172 [==============================] - 1s 5ms/step - loss: 0.3331 - accuracy: 0.8587 - val_loss: 0.4459 - val_accuracy: 0.7980\n",
            "Epoch 24/30\n",
            "172/172 [==============================] - 1s 5ms/step - loss: 0.3289 - accuracy: 0.8594 - val_loss: 0.4451 - val_accuracy: 0.8045\n",
            "Epoch 25/30\n",
            "172/172 [==============================] - 1s 5ms/step - loss: 0.3225 - accuracy: 0.8643 - val_loss: 0.4460 - val_accuracy: 0.8009\n",
            "Epoch 26/30\n",
            "172/172 [==============================] - 1s 5ms/step - loss: 0.3091 - accuracy: 0.8700 - val_loss: 0.4495 - val_accuracy: 0.8009\n",
            "Epoch 27/30\n",
            "172/172 [==============================] - 1s 6ms/step - loss: 0.2974 - accuracy: 0.8767 - val_loss: 0.4548 - val_accuracy: 0.8016\n",
            "Epoch 28/30\n",
            "172/172 [==============================] - 1s 5ms/step - loss: 0.2971 - accuracy: 0.8776 - val_loss: 0.4563 - val_accuracy: 0.8038\n",
            "Epoch 29/30\n",
            "172/172 [==============================] - 1s 5ms/step - loss: 0.2902 - accuracy: 0.8800 - val_loss: 0.4607 - val_accuracy: 0.8001\n",
            "Epoch 30/30\n",
            "172/172 [==============================] - 1s 5ms/step - loss: 0.2752 - accuracy: 0.8842 - val_loss: 0.4649 - val_accuracy: 0.8009\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HCApGM3rpf4P",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "6b1df440-68f9-43a3-a87b-cbb8469d79d0"
      },
      "source": [
        "cnn.load_weights('best_model_cnn.h5')\n",
        "\n",
        "y_pred = cnn.predict([x_test, meta_test], batch_size=32, verbose=2)\n",
        "y_pred = np.argmax(y_pred,axis=1)\n",
        "\n",
        "f1_score(real_test['target'], y_pred,average='micro')\n",
        "\n",
        "df = pd.DataFrame()\n",
        "df['id'] = [int(x) for x in id_test.values]\n",
        "df['target'] = y_pred\n",
        "df.to_csv('cnn.csv', index=False)\n",
        "from google.colab import files\n",
        "files.download('cnn.csv')"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "102/102 - 0s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_a143b34d-825a-42a2-9618-44d12ac518d6\", \"cnn.csv\", 22746)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZOtIG_1j7ja",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "8fb6c484-2768-41d6-a3f7-0bc6a2b5431f"
      },
      "source": [
        "f1_score(real_test['target'], y_pred,average='micro')\n"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8023291449586272"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    }
  ]
}